{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylangacq as pla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_cookie = torch.load('cookie_data_targets.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cookie = torch.load('cookie_target_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "552"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_cookie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_sentence = torch.load('sentence_data_targets.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sentence = torch.load('sentence_target_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cookie_filenames = torch.load('cookie_fnames_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "552"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_cookie_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sent_filenames = torch.load('sentence_fnames_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_sent_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(X_cookie_filenames) + len(X_sent_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cookie_reader = pla.read_chat('./Pitt/*/cookie/*.cha')\n",
    "X_sentence_reader = pla.read_chat('./Pitt/*/sentence/*.cha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data -- targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group missing or input error st no mean to replace with and left untouched in target generation\n",
    "\n",
    "replace with closest analog as per means generated from target generation:\n",
    "\n",
    "cookie {'Control': 30, 'ProbableAD': 19, 'MCI': 28, 'Memory': 30, 'Vascular': 17, 'PossibleAD': 20, 'Probable': 19, 'Other': 24}\n",
    "\n",
    "sentence {'ProbableAD': 19, 'MCI': 28, 'Memory': 30, 'Vascular': 17, 'PossibleAD': 21, 'Control': 30, 'Probable': 19, 'Other': 24}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent_k in y_sentence:\n",
    "    if type(y_sentence[sent_k]) == str:\n",
    "        print(X_sent_filenames[sent_k])\n",
    "        print(sent_k,y_sentence[sent_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_674 = X_sentence_reader.headers()['/Users/renee/Documents/WIS_Spr20/DL/FinalProj/Pitt/Dementia/sentence/236-0.cha']['Participants']['PAR']['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_674"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dementia'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_674"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Dementia with Possible mean (as per diagnostic codes in Pitt-data.xlxs and accompanying Pitt-readme.pdf)\n",
    "y_sentence[674] = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/renee/Documents/WIS_Spr20/DL/FinalProj/Pitt/Control/cookie/304-1.cha\n",
      "219 \n",
      "/Users/renee/Documents/WIS_Spr20/DL/FinalProj/Pitt/Dementia/cookie/585-0.cha\n",
      "511 possibleAD\n"
     ]
    }
   ],
   "source": [
    "for coo_kie in y_cookie:\n",
    "    if type(y_cookie[coo_kie]) == str:\n",
    "        print(X_cookie_filenames[coo_kie])\n",
    "        print(coo_kie,y_cookie[coo_kie])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_219 = X_cookie_reader.headers()['/Users/renee/Documents/WIS_Spr20/DL/FinalProj/Pitt/Control/cookie/304-1.cha']['Participants']['PAR']['education']\n",
    "group_219 = X_cookie_reader.headers()['/Users/renee/Documents/WIS_Spr20/DL/FinalProj/Pitt/Control/cookie/304-1.cha']['Participants']['PAR']['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '' with Control mean (as per folder and dx codes in .xlxs)\n",
    "y_cookie[219] = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_511 = X_cookie_reader.headers()['/Users/renee/Documents/WIS_Spr20/DL/FinalProj/Pitt/Dementia/cookie/585-0.cha']['Participants']['PAR']['education']\n",
    "group_511 = X_cookie_reader.headers()['/Users/renee/Documents/WIS_Spr20/DL/FinalProj/Pitt/Dementia/cookie/585-0.cha']['Participants']['PAR']['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'possibleAD'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace possibleAD with PossibleAD mean\n",
    "y_cookie[511] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm all targets filled now \n",
    "for sent_k in y_sentence:\n",
    "    if type(y_sentence[sent_k]) == str:\n",
    "        print(X_sent_filenames[sent_k])\n",
    "        print(sent_k,y_sentence[sent_k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coo_kie in y_cookie:\n",
    "    if type(y_cookie[coo_kie]) == str:\n",
    "        print(X_cookie_filenames[coo_kie])\n",
    "        print(coo_kie,y_cookie[coo_kie])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(y_cookie,'cookie_target_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(y_sentence,'sentence_target_dict.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Vocab Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = './PretrainedWordEmb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_emb = pickle.load(open(f'{glove_path}/addb.vocab_emb.glove.42B.300.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2188"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = torch.load('pos_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_ix = random.sample(range(num_samples),int(.20*num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ix = [i for i in range(num_samples) if i not in valid_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "634"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(valid_ix,'valid_ix.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(train_ix,'train_ix.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ix = torch.load('valid_ix.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ix = torch.load('train_ix.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "634"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item(ix=None,valid=False):\n",
    "    \n",
    "    # first should be...all zeros: each index represents greater than .. but you cant have a zero score..\n",
    "    def num_vectorize(t):\n",
    "\n",
    "        v = torch.zeros([29])\n",
    "        v[:t-1] = 1\n",
    "\n",
    "        return v.double()\n",
    "    \n",
    "    ix = random.randint(1,num_samples-1) if ix is None else ix\n",
    "    \n",
    "    #index, filename\n",
    "    if ix in X_cookie_filenames.keys():\n",
    "        _file = (ix,X_cookie_filenames[ix])\n",
    "        _reader = X_cookie_reader\n",
    "        _targetdict = y_cookie\n",
    "        \n",
    "    else:\n",
    "        _file = (ix,X_sent_filenames[ix])\n",
    "        _reader = X_sentence_reader\n",
    "        _targetdict = y_sentence\n",
    "        \n",
    "    file,data,targetdict = (_file,_reader,_targetdict)\n",
    "    \n",
    "    embeddings = []\n",
    "    targets = []\n",
    "\n",
    "    embedding = [(vocab_emb[token],torch.zeros(len(pos_dict),dtype=torch.float64),pos_dict[pos]) for (token,pos) in zip([tokensraw for utterance in data.tagged_sents(participant='PAR',by_files=True)[file[1]] for (tokensraw,pos,tokenstem,dependency) in utterance], [pos for utterance in data.tagged_sents(participant='PAR',by_files=True)[file[1]] for (tokensraw,pos,tokenstem,dependency) in utterance])]\n",
    "    target = targetdict[ix]\n",
    "\n",
    "    for tkn in embedding:\n",
    "\n",
    "        tkn[1][tkn[2]] = 1\n",
    "#     print(target)   \n",
    "    if valid: return [torch.cat((tkn[0],tkn[1])) for tkn in embedding], torch.tensor(target)\n",
    "    return [torch.cat((tkn[0],tkn[1])) for tkn in embedding], num_vectorize(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_minibatch(minib):\n",
    "    \n",
    "    batchsize = len(minib)\n",
    "    \n",
    "    seq_lens = [len(emb) for emb in minib]\n",
    "    max_len = max(seq_lens)\n",
    "    \n",
    "    # input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
    "#     seq_tensor = torch.zeros((batchsize, max_len, EMBEDDING_SIZE)).double()\n",
    "    seq_tensor = torch.zeros((max_len, batchsize, EMBEDDING_SIZE)).double()\n",
    "    \n",
    "                  \n",
    "    for i, (seq,length) in enumerate( zip(minib,seq_lens) ):\n",
    "        for wi,word in enumerate(seq):\n",
    "#             seq_tensor[i,wi] = word\n",
    "            seq_tensor[wi,i] = word\n",
    "    # mod to sort at the start?\n",
    "    seq_tensor = nn.utils.rnn.pack_padded_sequence(seq_tensor,lengths=seq_lens,enforce_sorted=False)\n",
    "    return seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad with zeros -- maybe\n",
    "# end with eos tensor, tag\n",
    "\n",
    "def get_minibatch(batchsize=1, ix=None):\n",
    "    \n",
    "    \n",
    "    def num_vectorize(targets):\n",
    "        vectors = []\n",
    "        for i in targets:\n",
    "            v = torch.zeros([30])\n",
    "            v[:i] = 1\n",
    "            vectors.append(v)\n",
    "        return vectors\n",
    "    \n",
    "    \n",
    "    minibatch_ix = random.sample(range(num_samples),batchsize) if ix is None else ix \n",
    "    \n",
    "    #index, filename\n",
    "    cookie_files = [(i,X_cookie_filenames[i]) for i in minibatch_ix if i in X_cookie_filenames.keys()]\n",
    "    sent_files = [(i,X_sent_filenames[i] )for i in minibatch_ix if i in X_sent_filenames.keys()]\n",
    "    \n",
    "    embeddings = []\n",
    "    targets = []\n",
    "\n",
    "    for corpus,data,targetdict in [(cookie_files,X_cookie_reader,y_cookie),(sent_files,X_sentence_reader,y_sentence)]: \n",
    "        for file_ix,file in corpus:\n",
    "#             print(file_ix, file)\n",
    "#             print('Words',[tokensraw for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance])\n",
    "#             print('Words_len',len([tokensraw for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance]))\n",
    "            embedding = [(vocab_emb[token],torch.zeros(len(pos_dict),dtype=torch.float64),pos_dict[pos]) for (token,pos) in zip([tokensraw for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance], [pos for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance])]\n",
    "            target = targetdict[file_ix]\n",
    "            \n",
    "            for tkn in embedding:\n",
    "\n",
    "                tkn[1][tkn[2]] = 1\n",
    "\n",
    "#                 embeddings.append(torch.cat((tkn[0],tkn[1])))\n",
    "            \n",
    "            embeddings.append([torch.cat((tkn[0],tkn[1])) for tkn in embedding])\n",
    "\n",
    "\n",
    "#             embeddings.append(embedding)\n",
    "            targets.append(target)\n",
    "#     print(targets)       \n",
    "#     return embeddings, torch.tensor(targets), minibatch_ix\n",
    "#     return embeddings, torch.tensor(targets)\n",
    "\n",
    "# return target vals for accuracy \n",
    "    return embeddings, num_vectorize(targets), torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/simple-trick-to-train-an-ordinal-regression-with-any-classifier-6911183d2a3c\n",
    "# validate -- accuracy \n",
    "def out_to_score_proba(yhat_vector):\n",
    "    # 29-dim sized vector\n",
    "    proba_vector = torch.zeros([30])\n",
    "    # no zero-score\n",
    "    proba_vector[0] = 0. \n",
    "    # probability == 1 , 1-P(y>1)   \n",
    "    proba_vector[1] = 1-yhat_vector[0]\n",
    "    \n",
    "    for i in range(2,len(yhat_vector)):\n",
    "        proba_vector[i] = yhat_vector[i-1] - yhat_vector[i]\n",
    "        \n",
    "    proba_vector[-1] = yhat_vector[-1] \n",
    "    \n",
    "#     print(proba_vector)\n",
    "    return torch.argmax(proba_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_to_score_proba(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shouldnt be on y but on targets ...\n",
    "def accuracy_distansum(yhat_tensor,y_tensor):\n",
    "    return (torch.abs(y - yhat).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix to rate\n",
    "def accuracy(yhat_tensor,y_tensor):\n",
    "    return torch.stack([ya==yb for (ya,yb) in zip(yhat_tensor,y_tensor)]).sum()\n",
    "    return ((y_hat == y_tensor).sum())/len(y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy measurement\n",
    "# return summation -- doesnt perfectly handle skipped values\n",
    "# np.where(vector)[0][-1] https://stackoverflow.com/questions/38375401/neural-network-ordinal-classification-for-age\n",
    "def out_to_score_summation(vector):\n",
    "    # zeros array \n",
    "    \n",
    "    return vector.sum() + 1\n",
    "    \n",
    "    if len(np.where(vector)[0]) == 0: return 0\n",
    "#     print ([i+2 for i in np.where(vector)])\n",
    "    print(np.where(vector))\n",
    "    candidate = (np.where(vector)[0][-1]) \n",
    "    \n",
    "    # handling intermediate zeros...maybe not optimal\n",
    "    if np.where(vector)[0][-2] < (candidate - 1): candidate = (np.where(vector)[0][-2] + np.where(vector)[0][-1])/2\n",
    "        \n",
    "    return candidate + 1\n",
    "    # alternative\n",
    "    return vector.sum() + 1\n",
    "#     len(np.where(t)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch of all samples:  longest sample length (for padding, if minibatch)\n",
    "max([len(emb) for emb in e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{371}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all words same len of emb size 371 \n",
    "set([len(emb[i]) for emb in e for i in range(len(emb))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq -- with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 layer rnn.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "# EMBEDDING_SIZE = 371\n",
    "# with EOS pos\n",
    "EMBEDDING_SIZE = len(vocab_emb['I']) + len(pos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max([len(emb) for emb in e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change if aadd in EOS, pos to 547\n",
    "MAX_LENGTH = 546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_dict['EOS'] = len(pos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EOS tag\n",
    "# vocab_emb['EOS'] = torch.rand([300],dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size=MAX_LENGTH, emb_size=EMBEDDING_SIZE, hidden_size=HIDDEN_SIZE):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # what is the input size fo self.embedding...? and is self.embedding necessary\n",
    "#         self.embedding = nn.Embedding(input_size, emb_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "#         self.gru = nn.GRU(input_size=self.emb_size, hidden_size=self.hidden_size,num_layers=2)\n",
    "# batch_first – If True, then the input and output tensors are provided as (batch, seq, feature) https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
    "        self.gru = nn.GRU(input_size=self.emb_size,hidden_size=self.hidden_size,num_layers=2).double()\n",
    "#         # each output node Oi of our neural network\n",
    "# uses a standard sigmoid function 1\n",
    "# 1+e−zi\n",
    "# , without including\n",
    "# the outputs from other nodes, as shown in Figure 1. Output\n",
    "# node Oi\n",
    "# is used to estimate the probability oi\n",
    "# that a data\n",
    "# point belongs to category i independently, without subjecting\n",
    "# to normalization as traditional neural networks do. Thus,\n",
    "# for a data point x of category k, the target vector is\n",
    "# (1, , 1, .., 1, 0, 0, 0), in which the first k elements is 1 and\n",
    "# others 0\n",
    "# http://orca.st.usm.edu/~zwang/files/rank.pdf A Neural Network Approach to Ordinal Regression\n",
    "    \n",
    "        self.fc = nn.Linear(self.hidden_size*2,30).double()\n",
    "        \n",
    "        self.activation = nn.Sigmoid()\n",
    "#         self.activations_list = [nn.Sigmoid() for i in range(30)]\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # confirm dims here...\n",
    "#         embedded = self.embedding(input).view(1, 1, -1)\n",
    "        \n",
    "        embedded = input.double()\n",
    "#         embedded = torch.cat(input).view(len(input), batch_size, -1).double()\n",
    "        \n",
    "#         output = embedded\n",
    "        \n",
    "#         output, hidden = self.gru(self.emb_size, self.hidden_size)\n",
    "#         output, hidden = self.gru(output)\n",
    "        gru_out, _ = self.gru(embedded,hidden)\n",
    "        \n",
    "        hid_out = torch.cat((_[-2,:,:], _[-1,:,:]), dim = 1)\n",
    "#         print(gru_out.size(),_.size())\n",
    "    \n",
    "#         lstm_out, _ = self.lstm(embedding.view(len(sentence), -1))\n",
    "        out = self.fc(hid_out)\n",
    "        print(out.size())\n",
    "        \n",
    "#         out = torch.tensor([a(o) for (a,o) in zip(self.activations_list,out[0])],requires_grad=True).double()\n",
    "        out - self.activation(out)\n",
    "        \n",
    "        # any further processing -- int? softmax...? \n",
    "#         return out\n",
    "        return out, _\n",
    "        return torch.nn.utils.rnn.pad_packed_sequence(out), _\n",
    "\n",
    "    def initHidden(self,batch_size=BATCH_SIZE):\n",
    "        # h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. If the LSTM is bidirectional, num_directions should be 2\n",
    "        return torch.zeros(2, batch_size, self.hidden_size, device=device).double()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The\n",
    "# cost function for a data point x can be relative entropy\n",
    "# or square error between the target vector and the output\n",
    "# vector\n",
    "# loss = nn.BCELoss()\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO minibatch + pad. For starters, stochastic gd.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hidden = encoder.initHidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([ya==yb for (ya,yb) in zip(y,yhat)]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 30])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 30])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 30])"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for h in yhat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(29)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_to_score_proba(yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29, 29, 29, 29, 28, 29, 29, 24, 29, 29, 29, 29, 29, 29, 29, 29, 28, 19,\n",
       "        19, 29, 29,  7, 28, 19, 28, 19, 19, 28, 12, 17, 19, 28])"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([out_to_score_proba(hat) for hat in yhat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30, 29, 24, 29, 25, 30, 30, 18, 30, 30, 29, 18, 16, 28,  3, 29, 18, 11,\n",
       "        19, 30, 30,  7, 29, 24, 21, 11, 21, 27, 10, 19, 11, 28])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "torch.Size([32, 30])\n",
      "step 1\n",
      "loss tensor(0.8583, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(25.0028, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(0)\n",
      "torch.Size([32, 30])\n",
      "step 2\n",
      "loss tensor(0.1608, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(9.5660, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(1)\n",
      "torch.Size([32, 30])\n",
      "step 3\n",
      "loss tensor(0.4708, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(17.2547, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(0)\n",
      "torch.Size([32, 30])\n",
      "step 4\n",
      "loss tensor(1.3132, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(31.5136, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(0)\n",
      "torch.Size([32, 30])\n",
      "step 5\n",
      "loss tensor(0.2835, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(12.1988, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(0)\n",
      "torch.Size([32, 30])\n",
      "step 6\n",
      "loss tensor(0.3871, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(15.5001, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(1)\n",
      "torch.Size([32, 30])\n",
      "step 7\n",
      "loss tensor(0.1539, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(9.6280, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(0)\n",
      "torch.Size([32, 30])\n",
      "step 8\n",
      "loss tensor(0.2153, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(11.4425, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(0)\n",
      "torch.Size([32, 30])\n",
      "step 9\n",
      "loss tensor(0.1849, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(10.5634, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(2)\n",
      "torch.Size([32, 30])\n",
      "step 10\n",
      "loss tensor(0.1468, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(9.0703, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(2)\n",
      "torch.Size([32, 30])\n",
      "step 11\n",
      "loss tensor(0.2084, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(10.3138, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(0)\n",
      "torch.Size([32, 30])\n",
      "step 12\n",
      "loss tensor(0.1550, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(8.9971, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(1)\n",
      "torch.Size([32, 30])\n",
      "step 13\n",
      "loss tensor(0.1395, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(8.4840, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(1)\n",
      "torch.Size([32, 30])\n",
      "step 14\n",
      "loss tensor(0.1927, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(10.2734, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(0)\n",
      "torch.Size([32, 30])\n",
      "step 15\n",
      "loss tensor(0.1277, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(8.0528, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(2)\n",
      "torch.Size([32, 30])\n",
      "step 16\n",
      "loss tensor(0.1528, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(8.6914, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(2)\n",
      "torch.Size([32, 30])\n",
      "step 17\n",
      "loss tensor(0.1382, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(8.2172, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(2)\n",
      "torch.Size([32, 30])\n",
      "step 18\n",
      "loss tensor(0.1153, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(7.6303, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(0)\n",
      "torch.Size([32, 30])\n",
      "step 19\n",
      "loss tensor(0.1368, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(8.2361, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(1)\n",
      "Epoch 1\n",
      "torch.Size([32, 30])\n",
      "step 1\n",
      "loss tensor(0.1405, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(8.3067, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(1)\n",
      "torch.Size([32, 30])\n",
      "step 2\n",
      "loss tensor(0.1407, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(8.5291, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(0)\n",
      "torch.Size([32, 30])\n",
      "step 3\n",
      "loss tensor(0.1164, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(7.7087, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(3)\n",
      "torch.Size([32, 30])\n",
      "step 4\n",
      "loss tensor(0.1286, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(7.7831, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(0)\n",
      "torch.Size([32, 30])\n",
      "step 5\n",
      "loss tensor(0.1085, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(7.2339, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(6)\n",
      "torch.Size([32, 30])\n",
      "step 6\n",
      "loss tensor(0.1039, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.9190, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(1)\n",
      "torch.Size([32, 30])\n",
      "step 7\n",
      "loss tensor(0.1156, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(7.3084, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(0)\n",
      "torch.Size([32, 30])\n",
      "step 8\n",
      "loss tensor(0.0980, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.7197, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(1)\n",
      "torch.Size([32, 30])\n",
      "step 9\n",
      "loss tensor(0.1005, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.9566, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(1)\n",
      "torch.Size([32, 30])\n",
      "step 10\n",
      "loss tensor(0.1099, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(7.1997, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(4)\n",
      "torch.Size([32, 30])\n",
      "step 11\n",
      "loss tensor(0.1189, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(7.5311, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(2)\n",
      "torch.Size([32, 30])\n",
      "step 12\n",
      "loss tensor(0.0965, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.9903, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(9)\n",
      "torch.Size([32, 30])\n",
      "step 13\n",
      "loss tensor(0.1171, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(7.5108, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(1)\n",
      "torch.Size([32, 30])\n",
      "step 14\n",
      "loss tensor(0.0984, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.9842, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(5)\n",
      "torch.Size([32, 30])\n",
      "step 15\n",
      "loss tensor(0.0990, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.6839, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(5)\n",
      "torch.Size([32, 30])\n",
      "step 16\n",
      "loss tensor(0.1076, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.7540, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(2)\n",
      "torch.Size([32, 30])\n",
      "step 17\n",
      "loss tensor(0.1116, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.9250, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(4)\n",
      "torch.Size([32, 30])\n",
      "step 18\n",
      "loss tensor(0.1259, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(7.7960, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(3)\n",
      "torch.Size([32, 30])\n",
      "step 19\n",
      "loss tensor(0.0959, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(7.0571, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(4)\n",
      "Epoch 2\n",
      "torch.Size([32, 30])\n",
      "step 1\n",
      "loss tensor(0.1160, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(7.4514, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(2)\n",
      "torch.Size([32, 30])\n",
      "step 2\n",
      "loss tensor(0.1015, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.4104, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(2)\n",
      "torch.Size([32, 30])\n",
      "step 3\n",
      "loss tensor(0.1247, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(7.1827, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(4)\n",
      "torch.Size([32, 30])\n",
      "step 4\n",
      "loss tensor(0.1107, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.7257, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(4)\n",
      "torch.Size([32, 30])\n",
      "step 5\n",
      "loss tensor(0.0859, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.1130, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(6)\n",
      "torch.Size([32, 30])\n",
      "step 6\n",
      "loss tensor(0.0871, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.2850, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(4)\n",
      "torch.Size([32, 30])\n",
      "step 7\n",
      "loss tensor(0.0728, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.1548, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(4)\n",
      "torch.Size([32, 30])\n",
      "step 8\n",
      "loss tensor(0.0941, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.5713, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(5)\n",
      "torch.Size([32, 30])\n",
      "step 9\n",
      "loss tensor(0.0850, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.8968, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(6)\n",
      "torch.Size([32, 30])\n",
      "step 10\n",
      "loss tensor(0.1004, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.1884, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(2)\n",
      "torch.Size([32, 30])\n",
      "step 11\n",
      "loss tensor(0.0950, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.1369, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(2)\n",
      "torch.Size([32, 30])\n",
      "step 12\n",
      "loss tensor(0.1101, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.4173, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(2)\n",
      "torch.Size([32, 30])\n",
      "step 13\n",
      "loss tensor(0.0859, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.2380, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(4)\n",
      "torch.Size([32, 30])\n",
      "step 14\n",
      "loss tensor(0.0783, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.1081, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(4)\n",
      "torch.Size([32, 30])\n",
      "step 15\n",
      "loss tensor(0.0868, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.4078, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(4)\n",
      "torch.Size([32, 30])\n",
      "step 16\n",
      "loss tensor(0.1087, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.4047, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(7)\n",
      "torch.Size([32, 30])\n",
      "step 17\n",
      "loss tensor(0.0822, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.7765, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(5)\n",
      "torch.Size([32, 30])\n",
      "step 18\n",
      "loss tensor(0.0805, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.7127, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(4)\n",
      "torch.Size([32, 30])\n",
      "step 19\n",
      "loss tensor(0.0804, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.6961, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(6)\n",
      "Epoch 3\n",
      "torch.Size([32, 30])\n",
      "step 1\n",
      "loss tensor(0.0807, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.5568, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(7)\n",
      "torch.Size([32, 30])\n",
      "step 2\n",
      "loss tensor(0.0844, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.7917, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(4)\n",
      "torch.Size([32, 30])\n",
      "step 3\n",
      "loss tensor(0.0932, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(6.3908, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(5)\n",
      "torch.Size([32, 30])\n",
      "step 4\n",
      "loss tensor(0.0767, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.4789, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(7)\n",
      "torch.Size([32, 30])\n",
      "step 5\n",
      "loss tensor(0.0784, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.7288, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(4)\n",
      "torch.Size([32, 30])\n",
      "step 6\n",
      "loss tensor(0.0820, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.6017, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(5)\n",
      "torch.Size([32, 30])\n",
      "step 7\n",
      "loss tensor(0.0735, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.6402, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(6)\n",
      "torch.Size([32, 30])\n",
      "step 8\n",
      "loss tensor(0.0667, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.3362, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(6)\n",
      "torch.Size([32, 30])\n",
      "step 9\n",
      "loss tensor(0.0823, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.6459, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(5)\n",
      "torch.Size([32, 30])\n",
      "step 10\n",
      "loss tensor(0.0845, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.7432, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(5)\n",
      "torch.Size([32, 30])\n",
      "step 11\n",
      "loss tensor(0.0749, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.5585, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(8)\n",
      "torch.Size([32, 30])\n",
      "step 12\n",
      "loss tensor(0.0788, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.8522, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(7)\n",
      "torch.Size([32, 30])\n",
      "step 13\n",
      "loss tensor(0.0725, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.3008, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(11)\n",
      "torch.Size([32, 30])\n",
      "step 14\n",
      "loss tensor(0.0648, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.9710, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(7)\n",
      "torch.Size([32, 30])\n",
      "step 15\n",
      "loss tensor(0.0741, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.4826, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(6)\n",
      "torch.Size([32, 30])\n",
      "step 16\n",
      "loss tensor(0.0645, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.8532, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(8)\n",
      "torch.Size([32, 30])\n",
      "step 17\n",
      "loss tensor(0.0718, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.3325, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(9)\n",
      "torch.Size([32, 30])\n",
      "step 18\n",
      "loss tensor(0.0696, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.2500, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(8)\n",
      "torch.Size([32, 30])\n",
      "step 19\n",
      "loss tensor(0.0724, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.5054, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(11)\n",
      "Epoch 4\n",
      "torch.Size([32, 30])\n",
      "step 1\n",
      "loss tensor(0.0742, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.3717, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(7)\n",
      "torch.Size([32, 30])\n",
      "step 2\n",
      "loss tensor(0.0679, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.9760, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(3)\n",
      "torch.Size([32, 30])\n",
      "step 3\n",
      "loss tensor(0.0755, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.2042, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(7)\n",
      "torch.Size([32, 30])\n",
      "step 4\n",
      "loss tensor(0.0752, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.0782, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(9)\n",
      "torch.Size([32, 30])\n",
      "step 5\n",
      "loss tensor(0.0611, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.9421, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(9)\n",
      "torch.Size([32, 30])\n",
      "step 6\n",
      "loss tensor(0.0572, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.8586, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(6)\n",
      "torch.Size([32, 30])\n",
      "step 7\n",
      "loss tensor(0.0634, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.0966, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(10)\n",
      "torch.Size([32, 30])\n",
      "step 8\n",
      "loss tensor(0.0780, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.4834, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(7)\n",
      "torch.Size([32, 30])\n",
      "step 9\n",
      "loss tensor(0.0703, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.4601, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(8)\n",
      "torch.Size([32, 30])\n",
      "step 10\n",
      "loss tensor(0.0694, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.6068, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(6)\n",
      "torch.Size([32, 30])\n",
      "step 11\n",
      "loss tensor(0.0612, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.9895, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(9)\n",
      "torch.Size([32, 30])\n",
      "step 12\n",
      "loss tensor(0.0664, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.8460, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(14)\n",
      "torch.Size([32, 30])\n",
      "step 13\n",
      "loss tensor(0.0739, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.1414, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(10)\n",
      "torch.Size([32, 30])\n",
      "step 14\n",
      "loss tensor(0.0688, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.9710, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(11)\n",
      "torch.Size([32, 30])\n",
      "step 15\n",
      "loss tensor(0.0679, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.7831, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(8)\n",
      "torch.Size([32, 30])\n",
      "step 16\n",
      "loss tensor(0.0551, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.6253, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(7)\n",
      "torch.Size([32, 30])\n",
      "step 17\n",
      "loss tensor(0.0610, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.8626, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(9)\n",
      "torch.Size([32, 30])\n",
      "step 18\n",
      "loss tensor(0.0696, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.2834, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(8)\n",
      "torch.Size([32, 30])\n",
      "step 19\n",
      "loss tensor(0.0513, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.6304, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(6)\n",
      "Epoch 5\n",
      "torch.Size([32, 30])\n",
      "step 1\n",
      "loss tensor(0.0763, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.4208, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(6)\n",
      "torch.Size([32, 30])\n",
      "step 2\n",
      "loss tensor(0.0552, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.6276, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(10)\n",
      "torch.Size([32, 30])\n",
      "step 3\n",
      "loss tensor(0.0551, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.6096, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(13)\n",
      "torch.Size([32, 30])\n",
      "step 4\n",
      "loss tensor(0.0414, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.1590, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(15)\n",
      "torch.Size([32, 30])\n",
      "step 5\n",
      "loss tensor(0.0851, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.7543, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(7)\n",
      "torch.Size([32, 30])\n",
      "step 6\n",
      "loss tensor(0.0474, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.2545, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(10)\n",
      "torch.Size([32, 30])\n",
      "step 7\n",
      "loss tensor(0.0661, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.1830, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(8)\n",
      "torch.Size([32, 30])\n",
      "step 8\n",
      "loss tensor(0.0758, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.7007, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(4)\n",
      "torch.Size([32, 30])\n",
      "step 9\n",
      "loss tensor(0.0544, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.7064, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(11)\n",
      "torch.Size([32, 30])\n",
      "step 10\n",
      "loss tensor(0.0558, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.7344, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(6)\n",
      "torch.Size([32, 30])\n",
      "step 11\n",
      "loss tensor(0.0720, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.3807, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(7)\n",
      "torch.Size([32, 30])\n",
      "step 12\n",
      "loss tensor(0.0691, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.9855, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(6)\n",
      "torch.Size([32, 30])\n",
      "step 13\n",
      "loss tensor(0.0777, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.4522, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(5)\n",
      "torch.Size([32, 30])\n",
      "step 14\n",
      "loss tensor(0.0709, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.4948, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(8)\n",
      "torch.Size([32, 30])\n",
      "step 15\n",
      "loss tensor(0.0607, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.1058, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(8)\n",
      "torch.Size([32, 30])\n",
      "step 16\n",
      "loss tensor(0.0631, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.1120, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(6)\n",
      "torch.Size([32, 30])\n",
      "step 17\n",
      "loss tensor(0.0646, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.0735, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(6)\n",
      "torch.Size([32, 30])\n",
      "step 18\n",
      "loss tensor(0.0570, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.6820, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(6)\n",
      "torch.Size([32, 30])\n",
      "step 19\n",
      "loss tensor(0.0512, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.5010, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(12)\n",
      "Epoch 6\n",
      "torch.Size([32, 30])\n",
      "step 1\n",
      "loss tensor(0.0647, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.1662, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(6)\n",
      "torch.Size([32, 30])\n",
      "step 2\n",
      "loss tensor(0.0514, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.6552, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(9)\n",
      "torch.Size([32, 30])\n",
      "step 3\n",
      "loss tensor(0.0677, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(5.2630, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(8)\n",
      "torch.Size([32, 30])\n",
      "step 4\n",
      "loss tensor(0.0401, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.0536, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(7)\n",
      "torch.Size([32, 30])\n",
      "step 5\n",
      "loss tensor(0.0638, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.9269, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(8)\n",
      "torch.Size([32, 30])\n",
      "step 6\n",
      "loss tensor(0.0508, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.4555, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(13)\n",
      "torch.Size([32, 30])\n",
      "step 7\n",
      "loss tensor(0.0444, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.3395, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(8)\n",
      "torch.Size([32, 30])\n",
      "step 8\n",
      "loss tensor(0.0551, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.5550, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(7)\n",
      "torch.Size([32, 30])\n",
      "step 9\n",
      "loss tensor(0.0487, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.4534, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(11)\n",
      "torch.Size([32, 30])\n",
      "step 10\n",
      "loss tensor(0.0459, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.2676, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(14)\n",
      "torch.Size([32, 30])\n",
      "step 11\n",
      "loss tensor(0.0527, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.5612, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(10)\n",
      "torch.Size([32, 30])\n",
      "step 12\n",
      "loss tensor(0.0555, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.5524, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(11)\n",
      "torch.Size([32, 30])\n",
      "step 13\n",
      "loss tensor(0.0540, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.5073, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(12)\n",
      "torch.Size([32, 30])\n",
      "step 14\n",
      "loss tensor(0.0452, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.3317, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(9)\n",
      "torch.Size([32, 30])\n",
      "step 15\n",
      "loss tensor(0.0559, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.6661, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(4)\n",
      "torch.Size([32, 30])\n",
      "step 16\n",
      "loss tensor(0.0457, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.3644, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(8)\n",
      "torch.Size([32, 30])\n",
      "step 17\n",
      "loss tensor(0.0588, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.7728, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(11)\n",
      "torch.Size([32, 30])\n",
      "step 18\n",
      "loss tensor(0.0463, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.3278, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(12)\n",
      "torch.Size([32, 30])\n",
      "step 19\n",
      "loss tensor(0.0344, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.7859, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(13)\n",
      "Epoch 7\n",
      "torch.Size([32, 30])\n",
      "step 1\n",
      "loss tensor(0.0410, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.0438, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(12)\n",
      "torch.Size([32, 30])\n",
      "step 2\n",
      "loss tensor(0.0355, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.8312, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(13)\n",
      "torch.Size([32, 30])\n",
      "step 3\n",
      "loss tensor(0.0479, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.2382, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(13)\n",
      "torch.Size([32, 30])\n",
      "step 4\n",
      "loss tensor(0.0481, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.1558, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(9)\n",
      "torch.Size([32, 30])\n",
      "step 5\n",
      "loss tensor(0.0376, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.9017, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(8)\n",
      "torch.Size([32, 30])\n",
      "step 6\n",
      "loss tensor(0.0443, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.1489, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(9)\n",
      "torch.Size([32, 30])\n",
      "step 7\n",
      "loss tensor(0.0500, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.3105, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(14)\n",
      "torch.Size([32, 30])\n",
      "step 8\n",
      "loss tensor(0.0375, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.7815, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(11)\n",
      "torch.Size([32, 30])\n",
      "step 9\n",
      "loss tensor(0.0368, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.8317, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(17)\n",
      "torch.Size([32, 30])\n",
      "step 10\n",
      "loss tensor(0.0531, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.4418, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(8)\n",
      "torch.Size([32, 30])\n",
      "step 11\n",
      "loss tensor(0.0358, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.7822, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(14)\n",
      "torch.Size([32, 30])\n",
      "step 12\n",
      "loss tensor(0.0546, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.3284, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(9)\n",
      "torch.Size([32, 30])\n",
      "step 13\n",
      "loss tensor(0.0565, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.4807, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(11)\n",
      "torch.Size([32, 30])\n",
      "step 14\n",
      "loss tensor(0.0461, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.0462, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(14)\n",
      "torch.Size([32, 30])\n",
      "step 15\n",
      "loss tensor(0.0472, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.4823, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(14)\n",
      "torch.Size([32, 30])\n",
      "step 16\n",
      "loss tensor(0.0391, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.9120, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(15)\n",
      "torch.Size([32, 30])\n",
      "step 17\n",
      "loss tensor(0.0490, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.3463, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(7)\n",
      "torch.Size([32, 30])\n",
      "step 18\n",
      "loss tensor(0.0370, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.7948, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(12)\n",
      "torch.Size([32, 30])\n",
      "step 19\n",
      "loss tensor(0.0340, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.6894, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(13)\n",
      "Epoch 8\n",
      "torch.Size([32, 30])\n",
      "step 1\n",
      "loss tensor(0.0389, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.8993, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(13)\n",
      "torch.Size([32, 30])\n",
      "step 2\n",
      "loss tensor(0.0420, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.2089, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(13)\n",
      "torch.Size([32, 30])\n",
      "step 3\n",
      "loss tensor(0.0452, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.0590, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(9)\n",
      "torch.Size([32, 30])\n",
      "step 4\n",
      "loss tensor(0.0503, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(4.2431, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(11)\n",
      "torch.Size([32, 30])\n",
      "step 5\n",
      "loss tensor(0.0289, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.3983, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(17)\n",
      "torch.Size([32, 30])\n",
      "step 6\n",
      "loss tensor(0.0367, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.7232, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(10)\n",
      "torch.Size([32, 30])\n",
      "step 7\n",
      "loss tensor(0.0266, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.2606, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(15)\n",
      "torch.Size([32, 30])\n",
      "step 8\n",
      "loss tensor(0.0332, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.5985, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(12)\n",
      "torch.Size([32, 30])\n",
      "step 9\n",
      "loss tensor(0.0371, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.8752, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(9)\n",
      "torch.Size([32, 30])\n",
      "step 10\n",
      "loss tensor(0.0361, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.6318, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(16)\n",
      "torch.Size([32, 30])\n",
      "step 11\n",
      "loss tensor(0.0281, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.3069, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(18)\n",
      "torch.Size([32, 30])\n",
      "step 12\n",
      "loss tensor(0.0350, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.6564, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(12)\n",
      "torch.Size([32, 30])\n",
      "step 13\n",
      "loss tensor(0.0371, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.7672, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(14)\n",
      "torch.Size([32, 30])\n",
      "step 14\n",
      "loss tensor(0.0389, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.8193, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(14)\n",
      "torch.Size([32, 30])\n",
      "step 15\n",
      "loss tensor(0.0307, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.4475, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(14)\n",
      "torch.Size([32, 30])\n",
      "step 16\n",
      "loss tensor(0.0320, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.4035, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(14)\n",
      "torch.Size([32, 30])\n",
      "step 17\n",
      "loss tensor(0.0279, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.3793, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(16)\n",
      "torch.Size([32, 30])\n",
      "step 18\n",
      "loss tensor(0.0352, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.6890, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(19)\n",
      "torch.Size([32, 30])\n",
      "step 19\n",
      "loss tensor(0.0281, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.2869, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(14)\n",
      "Epoch 9\n",
      "torch.Size([32, 30])\n",
      "step 1\n",
      "loss tensor(0.0287, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.3050, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(16)\n",
      "torch.Size([32, 30])\n",
      "step 2\n",
      "loss tensor(0.0332, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.6054, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(14)\n",
      "torch.Size([32, 30])\n",
      "step 3\n",
      "loss tensor(0.0351, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.7258, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(15)\n",
      "torch.Size([32, 30])\n",
      "step 4\n",
      "loss tensor(0.0405, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.9366, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(12)\n",
      "torch.Size([32, 30])\n",
      "step 5\n",
      "loss tensor(0.0327, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.6112, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(16)\n",
      "torch.Size([32, 30])\n",
      "step 6\n",
      "loss tensor(0.0309, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.4605, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(9)\n",
      "torch.Size([32, 30])\n",
      "step 7\n",
      "loss tensor(0.0420, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.8258, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(12)\n",
      "torch.Size([32, 30])\n",
      "step 8\n",
      "loss tensor(0.0292, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.4389, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(19)\n",
      "torch.Size([32, 30])\n",
      "step 9\n",
      "loss tensor(0.0250, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.1708, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(14)\n",
      "torch.Size([32, 30])\n",
      "step 10\n",
      "loss tensor(0.0329, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.4031, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(14)\n",
      "torch.Size([32, 30])\n",
      "step 11\n",
      "loss tensor(0.0286, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.3512, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(18)\n",
      "torch.Size([32, 30])\n",
      "step 12\n",
      "loss tensor(0.0290, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.4973, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(17)\n",
      "torch.Size([32, 30])\n",
      "step 13\n",
      "loss tensor(0.0276, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.1372, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(15)\n",
      "torch.Size([32, 30])\n",
      "step 14\n",
      "loss tensor(0.0297, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.4378, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(17)\n",
      "torch.Size([32, 30])\n",
      "step 15\n",
      "loss tensor(0.0234, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.0103, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(17)\n",
      "torch.Size([32, 30])\n",
      "step 16\n",
      "loss tensor(0.0246, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.2061, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(21)\n",
      "torch.Size([32, 30])\n",
      "step 17\n",
      "loss tensor(0.0257, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.1155, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(17)\n",
      "torch.Size([32, 30])\n",
      "step 18\n",
      "loss tensor(0.0272, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.3066, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(11)\n",
      "torch.Size([32, 30])\n",
      "step 19\n",
      "loss tensor(0.0277, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "accuracy_distance tensor(3.3380, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "accuracy tensor(18)\n"
     ]
    }
   ],
   "source": [
    "encoder.train()\n",
    "# _hidden = encoder.initHidden()\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    # modify for ordering -- benefit here with shuffling at ea epoch\n",
    "    random.shuffle(train_ix)\n",
    "    \n",
    "    print('Epoch',epoch)\n",
    "    step = 0\n",
    "    \n",
    "    # in future, mix up epochs \n",
    "    for i in range(19):\n",
    "        \n",
    "        step += 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x,y,targets = get_minibatch(batchsize=BATCH_SIZE)\n",
    "        x = pad_minibatch(x)\n",
    "        \n",
    "        y = torch.stack(y).double()\n",
    "        \n",
    "        _hidden = encoder.initHidden(batch_size=BATCH_SIZE)\n",
    "\n",
    "        yhat, _hidden = encoder(x, _hidden)\n",
    "\n",
    "        loss = loss_func(yhat,y)\n",
    "\n",
    "        loss.backward()\n",
    "#         loss.backward(retain_graph=True)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "       \n",
    "        print('step',step)\n",
    "        print('loss',loss)\n",
    "\n",
    "        yhat_tensor = torch.tensor([out_to_score_proba(hat) for hat in yhat])\n",
    "        \n",
    "        print('accuracy_distance',accuracy_distansum(yhat_tensor=yhat_tensor,y_tensor=targets))\n",
    "        print('accuracy',accuracy(yhat_tensor=yhat_tensor,y_tensor=targets))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/renee/opt/anaconda3/envs/dl1010/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type EncoderRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/renee/opt/anaconda3/envs/dl1010/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/renee/opt/anaconda3/envs/dl1010/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/renee/opt/anaconda3/envs/dl1010/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sigmoid. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(encoder,'encoderv1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([158, 30])\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "\n",
    "x_valid,y_valid,targets_valid = get_minibatch(ix=valid_ix)\n",
    "\n",
    "x_valid = pad_minibatch(x_valid)\n",
    "        \n",
    "y_valid = torch.stack(y_valid).double()\n",
    "\n",
    "_hidden = encoder.initHidden(batch_size=len(valid_ix))\n",
    "\n",
    "yhat_valid, _hidden = encoder(x_valid, _hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29, 18, 29, 29, 26, 28, 23, 29, 22, 17, 20, 20, 25, 29, 28, 28, 29, 29,\n",
       "        19, 16, 29,  7, 28, 29, 28, 28, 28, 12, 29, 12, 29, 25, 19, 18, 28, 26,\n",
       "        27, 29, 20, 17, 29, 26, 18, 29, 19, 20, 20, 29, 28, 28, 29, 19, 28, 20,\n",
       "        22, 29, 19, 29, 29, 29, 29, 22, 19, 27, 28, 29, 29, 22, 29, 29, 29, 28,\n",
       "        29, 20, 29, 29, 29,  8, 29, 20, 29, 29, 29, 16, 28, 22, 29, 28, 16, 29,\n",
       "        29, 19,  3, 29, 19, 29, 29, 20, 29, 20, 29, 19, 12, 29, 20, 23, 13, 20,\n",
       "        23, 29, 29, 19, 20, 19, 19, 19, 27, 29, 10, 20, 18, 18, 24, 26, 16, 20,\n",
       "        18, 17, 28, 28, 29, 20, 27, 20, 29, 19, 20, 28, 29, 22, 18, 20, 20, 19,\n",
       "        19, 28, 28, 28, 26, 19, 24, 22, 18, 18, 20, 28, 25, 27])"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([out_to_score_proba(hat) for hat in yhat_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30, 18, 29, 28, 26, 27, 23, 30, 19, 17, 20, 17, 25, 30, 23, 29, 27, 30,\n",
       "        19, 16, 30,  7, 28, 28, 28, 28, 25, 12, 30, 18, 30, 25, 15, 18, 14, 26,\n",
       "        27, 30, 20, 17, 30, 26, 19, 27, 17, 20, 20, 29, 27, 29, 30, 30, 30, 20,\n",
       "        22, 29, 19, 30, 30, 30, 30, 22, 17, 27, 27, 30, 30, 23, 30, 30, 19, 25,\n",
       "        30, 19, 30, 30, 30,  8, 30, 22, 30, 30, 28, 16, 19, 22, 24, 28, 16, 30,\n",
       "        30, 19, 17, 27, 19, 30, 30, 21, 28, 19, 29, 19, 12, 28, 20, 23, 13, 20,\n",
       "        23, 27, 23, 19, 20, 19, 19, 19, 27, 16, 10, 20, 17, 24, 24, 23, 16, 20,\n",
       "        18, 17, 19, 28, 29, 20, 27, 20, 23, 16, 20, 23, 27, 22, 19, 20, 20, 19,\n",
       "        19, 28, 28, 28, 26, 19, 24, 22, 20, 18, 23, 27, 25, 25])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(78)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(torch.tensor([out_to_score_proba(hat) for hat in yhat_valid]) == targets_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4936708860759494"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "78/158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(106.8160, dtype=torch.float64, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_distansum(y_tensor=targets_valid,yhat_tensor=torch.tensor([out_to_score_proba(hat) for hat in yhat_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6708860759493671"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "106/158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = get_minibatch(batchsize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([55, 1, 371])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(x[0]).view(len(x[0]),1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to be able to pass any/all into net\n",
    "# encoder = EncoderRNN(len(x[0])).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 128])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_hidden.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python dl1010",
   "language": "python",
   "name": "dl1010"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
