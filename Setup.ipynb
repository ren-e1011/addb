{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylangacq as pla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_cookie = torch.load('cookie_data_targets.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cookie = torch.load('cookie_target_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "552"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_cookie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_sentence = torch.load('sentence_data_targets.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sentence = torch.load('sentence_target_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cookie_filenames = torch.load('cookie_fnames_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "552"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_cookie_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sent_filenames = torch.load('sentence_fnames_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_sent_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(X_cookie_filenames) + len(X_sent_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cookie_reader = pla.read_chat('./Pitt/*/cookie/*.cha')\n",
    "X_sentence_reader = pla.read_chat('./Pitt/*/sentence/*.cha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data -- targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group missing or input error st no mean to replace with and left untouched in target generation\n",
    "\n",
    "replace with closest analog as per means generated from target generation:\n",
    "\n",
    "cookie {'Control': 30, 'ProbableAD': 19, 'MCI': 28, 'Memory': 30, 'Vascular': 17, 'PossibleAD': 20, 'Probable': 19, 'Other': 24}\n",
    "\n",
    "sentence {'ProbableAD': 19, 'MCI': 28, 'Memory': 30, 'Vascular': 17, 'PossibleAD': 21, 'Control': 30, 'Probable': 19, 'Other': 24}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent_k in y_sentence:\n",
    "    if type(y_sentence[sent_k]) == str:\n",
    "        print(X_sent_filenames[sent_k])\n",
    "        print(sent_k,y_sentence[sent_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_674 = X_sentence_reader.headers()['/Users/renee/Documents/WIS_Spr20/DL/FinalProj/Pitt/Dementia/sentence/236-0.cha']['Participants']['PAR']['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_674"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dementia'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_674"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Dementia with Possible mean (as per diagnostic codes in Pitt-data.xlxs and accompanying Pitt-readme.pdf)\n",
    "y_sentence[674] = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/renee/Documents/WIS_Spr20/DL/FinalProj/Pitt/Control/cookie/304-1.cha\n",
      "219 \n",
      "/Users/renee/Documents/WIS_Spr20/DL/FinalProj/Pitt/Dementia/cookie/585-0.cha\n",
      "511 possibleAD\n"
     ]
    }
   ],
   "source": [
    "for coo_kie in y_cookie:\n",
    "    if type(y_cookie[coo_kie]) == str:\n",
    "        print(X_cookie_filenames[coo_kie])\n",
    "        print(coo_kie,y_cookie[coo_kie])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_219 = X_cookie_reader.headers()['/Users/renee/Documents/WIS_Spr20/DL/FinalProj/Pitt/Control/cookie/304-1.cha']['Participants']['PAR']['education']\n",
    "group_219 = X_cookie_reader.headers()['/Users/renee/Documents/WIS_Spr20/DL/FinalProj/Pitt/Control/cookie/304-1.cha']['Participants']['PAR']['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '' with Control mean (as per folder and dx codes in .xlxs)\n",
    "y_cookie[219] = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_511 = X_cookie_reader.headers()['/Users/renee/Documents/WIS_Spr20/DL/FinalProj/Pitt/Dementia/cookie/585-0.cha']['Participants']['PAR']['education']\n",
    "group_511 = X_cookie_reader.headers()['/Users/renee/Documents/WIS_Spr20/DL/FinalProj/Pitt/Dementia/cookie/585-0.cha']['Participants']['PAR']['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'possibleAD'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace possibleAD with PossibleAD mean\n",
    "y_cookie[511] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm all targets filled now \n",
    "for sent_k in y_sentence:\n",
    "    if type(y_sentence[sent_k]) == str:\n",
    "        print(X_sent_filenames[sent_k])\n",
    "        print(sent_k,y_sentence[sent_k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coo_kie in y_cookie:\n",
    "    if type(y_cookie[coo_kie]) == str:\n",
    "        print(X_cookie_filenames[coo_kie])\n",
    "        print(coo_kie,y_cookie[coo_kie])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(y_cookie,'cookie_target_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(y_sentence,'sentence_target_dict.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Vocab Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = './PretrainedWordEmb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_emb = pickle.load(open(f'{glove_path}/addb.vocab_emb.glove.42B.300.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2188"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = torch.load('pos_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_ix = random.sample(range(num_samples),int(.20*num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ix = [i for i in range(num_samples) if i not in valid_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(valid_ix,'valid_ix.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(train_ix,'train_ix.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ix = torch.load('valid_ix.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ix = torch.load('train_ix.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "634"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group minibatches of similar size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file index: sequence length\n",
    "ix_size_dict = {}\n",
    "minibatch_ix = train_ix\n",
    "    \n",
    "#index, filename\n",
    "cookie_files = [(i,X_cookie_filenames[i]) for i in minibatch_ix if i in X_cookie_filenames.keys()]\n",
    "sent_files = [(i,X_sent_filenames[i] )for i in minibatch_ix if i in X_sent_filenames.keys()]\n",
    "\n",
    "\n",
    "for corpus,data,targetdict in [(cookie_files,X_cookie_reader,y_cookie),(sent_files,X_sentence_reader,y_sentence)]: \n",
    "    for file_ix,file in corpus:\n",
    "#             print(file_ix, file)\n",
    "#             print('Words',[tokensraw for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance])\n",
    "#             print('Words_len',len([tokensraw for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance]))\n",
    "        embedding = [(vocab_emb[token],torch.zeros(len(pos_dict),dtype=torch.float64),pos_dict[pos]) for (token,pos) in zip([tokensraw for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance], [pos for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance])]\n",
    "        ix_size_dict[file_ix] = len(embedding)\n",
    "#         target = targetdict[file_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence length: [file indices]\n",
    "size_ix_dict = {}\n",
    "for k,v in ix_size_dict.items():\n",
    "    if v not in size_ix_dict.keys():\n",
    "        size_ix_dict[v] = [k]\n",
    "    else:\n",
    "        size_ix_dict[v].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of sequence-lengths\n",
    "len_sents = sorted(list(size_ix_dict.keys()),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-07747f6bed1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatchset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mminibsize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mbatchset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_ix_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mminibsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlen_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "batches = []\n",
    "while len(len_sents) > 0:\n",
    "    minibsize = 0\n",
    "    batchset = []\n",
    "    while minibsize < 25:\n",
    "        batchset.extend(size_ix_dict[len_sents[0]])\n",
    "        minibsize = len(batchset)\n",
    "        len_sents.pop(0)\n",
    "    batches.append(batchset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append final batchset\n",
    "batches.append(batchset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "634"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm all train files accounted for \n",
    "sum([1 for blist in batches for bitem in blist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "634"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad with zeros -- maybe\n",
    "# end with eos tensor, tag\n",
    "\n",
    "def get_minibatch(batchsize=1, ix=None,pad=True):\n",
    "    \n",
    "    \n",
    "    def num_vectorize(targets):\n",
    "        vectors = []\n",
    "        for i in targets:\n",
    "            v = torch.zeros([30])\n",
    "            # up until MMSE index...greater than index i \n",
    "            v[:i] = 1\n",
    "            vectors.append(v)\n",
    "        return torch.stack(vectors).double()\n",
    "    \n",
    "    def pad_minibatch(minib):\n",
    "    \n",
    "        batchsize = len(minib)\n",
    "\n",
    "        seq_lens = [len(emb) for emb in minib]\n",
    "        max_len = max(seq_lens)\n",
    "\n",
    "        # input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
    "    #     seq_tensor = torch.zeros((batchsize, max_len, EMBEDDING_SIZE)).double()\n",
    "        seq_tensor = torch.zeros((max_len, batchsize, EMBEDDING_SIZE)).double()\n",
    "\n",
    "\n",
    "        for i, (seq,length) in enumerate( zip(minib,seq_lens) ):\n",
    "            for wi,word in enumerate(seq):\n",
    "    #             seq_tensor[i,wi] = word\n",
    "                seq_tensor[wi,i] = word\n",
    "        # mod to sort at the start?\n",
    "        seq_tensor = nn.utils.rnn.pack_padded_sequence(seq_tensor,lengths=seq_lens,enforce_sorted=False)\n",
    "        return seq_tensor\n",
    "    \n",
    "    \n",
    "    minibatch_ix = random.sample(range(num_samples),batchsize) if ix is None else ix \n",
    "    \n",
    "    #index, filename\n",
    "    cookie_files = [(i,X_cookie_filenames[i]) for i in minibatch_ix if i in X_cookie_filenames.keys()]\n",
    "    sent_files = [(i,X_sent_filenames[i] )for i in minibatch_ix if i in X_sent_filenames.keys()]\n",
    "    \n",
    "    embeddings = []\n",
    "    targets = []\n",
    "\n",
    "    for corpus,data,targetdict in [(cookie_files,X_cookie_reader,y_cookie),(sent_files,X_sentence_reader,y_sentence)]: \n",
    "        for file_ix,file in corpus:\n",
    "#             print(file_ix, file)\n",
    "#             print('Words',[tokensraw for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance])\n",
    "#             print('Words_len',len([tokensraw for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance]))\n",
    "            embedding = [(vocab_emb[token],torch.zeros(len(pos_dict),dtype=torch.float64),pos_dict[pos]) for (token,pos) in zip([tokensraw for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance], [pos for utterance in data.tagged_sents(participant='PAR',by_files=True)[file] for (tokensraw,pos,tokenstem,dependency) in utterance])]\n",
    "            target = targetdict[file_ix]\n",
    "            \n",
    "            for tkn in embedding:\n",
    "\n",
    "                tkn[1][tkn[2]] = 1\n",
    "\n",
    "#                 embeddings.append(torch.cat((tkn[0],tkn[1])))\n",
    "            \n",
    "            embeddings.append([torch.cat((tkn[0],tkn[1])) for tkn in embedding])\n",
    "\n",
    "\n",
    "#             embeddings.append(embedding)\n",
    "            targets.append(target)\n",
    "#     print(targets)       \n",
    "#     return embeddings, torch.tensor(targets), minibatch_ix\n",
    "#     return embeddings, torch.tensor(targets)\n",
    "    if pad: embeddings = pad_minibatch(embeddings)\n",
    "# return target vals for accuracy \n",
    "    return embeddings, num_vectorize(targets), torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/simple-trick-to-train-an-ordinal-regression-with-any-classifier-6911183d2a3c\n",
    "# transform proba dist prediction to MMSE target\n",
    "def out_to_score_proba(yhat_vector):\n",
    "    # mod to 31 for ix 0-30\n",
    "    proba_vector = torch.zeros([31])\n",
    "    # no zero-score\n",
    "    proba_vector[0] = 0. \n",
    "# P(MMSE=i) = P(MMSE>i-1) - P(MMSE>i)\n",
    "    for i in range(1,len(yhat_vector)):\n",
    "        proba_vector[i] = yhat_vector[i-1] - yhat_vector[i]\n",
    "        \n",
    "    proba_vector[-1] = yhat_vector[-1] \n",
    "    \n",
    "#     print(proba_vector)\n",
    "    return torch.argmax(proba_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not on y but on targets as mmse scores\n",
    "def accuracy_distansum(yhat_tensor,y_tensor):\n",
    "    return (torch.abs(y - yhat).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw accurate prediction count\n",
    "def accuracy(yhat_tensor,y_tensor):\n",
    "#     return torch.stack([ya==yb for (ya,yb) in zip(yhat_tensor,y_tensor)]).sum()\n",
    "    return ((yhat_tensor == y_tensor).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-layer rnn: gru units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "# EMBEDDING_SIZE = 371\n",
    "EMBEDDING_SIZE = len(vocab_emb['I']) + len(pos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max([len(emb) for emb in e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change if aadd in EOS, pos to 547\n",
    "MAX_LENGTH = 546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size=MAX_LENGTH, emb_size=EMBEDDING_SIZE, hidden_size=HIDDEN_SIZE):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size=self.emb_size,hidden_size=self.hidden_size,num_layers=2).double()\n",
    "#         # each output node Oi of our neural network\n",
    "# uses a standard sigmoid function 1\n",
    "# 1+eâˆ’zi\n",
    "# , without including\n",
    "# the outputs from other nodes, as shown in Figure 1. Output\n",
    "# node Oi\n",
    "# is used to estimate the probability oi\n",
    "# that a data\n",
    "# point belongs to category i independently, without subjecting\n",
    "# to normalization as traditional neural networks do. Thus,\n",
    "# for a data point x of category k, the target vector is\n",
    "# (1, , 1, .., 1, 0, 0, 0), in which the first k elements is 1 and\n",
    "# others 0\n",
    "# http://orca.st.usm.edu/~zwang/files/rank.pdf A Neural Network Approach to Ordinal Regression\n",
    "    \n",
    "        self.fc = nn.Linear(self.hidden_size*2,30).double()\n",
    "        \n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        embedded = input.double()\n",
    "\n",
    "        gru_out, _ = self.gru(embedded,hidden)\n",
    "        \n",
    "        hid_out = torch.cat((_[-2,:,:], _[-1,:,:]), dim = 1)\n",
    "\n",
    "        out = self.fc(hid_out)\n",
    "#         print(out.size())\n",
    "\n",
    "        out - self.activation(out)\n",
    "\n",
    "        return out, _\n",
    "#         return torch.nn.utils.rnn.pad_packed_sequence(out), _\n",
    "\n",
    "    def initHidden(self,batch_size=BATCH_SIZE):\n",
    "        # h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. If the LSTM is bidirectional, num_directions should be 2\n",
    "        return torch.zeros(2, batch_size, self.hidden_size, device=device).double()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The\n",
    "# cost function for a data point x can be relative entropy\n",
    "# or square error between the target vector and the output\n",
    "# vector\n",
    "# loss = nn.BCELoss()\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25\n",
      "===========\n",
      "torch.Size([26, 30])\n",
      "step 1\n",
      "loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(31.8367, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([13, 30])\n",
      "step 2\n",
      "loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(15.9313, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(13)\n",
      "torch.Size([26, 30])\n",
      "step 3\n",
      "loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(32.3432, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([32, 30])\n",
      "step 4\n",
      "loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(37.2153, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(32)\n",
      "torch.Size([25, 30])\n",
      "step 5\n",
      "loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(27.8402, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([25, 30])\n",
      "step 6\n",
      "loss tensor(0.0042, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(31.7577, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([25, 30])\n",
      "step 7\n",
      "loss tensor(0.0045, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(35.0495, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([26, 30])\n",
      "step 8\n",
      "loss tensor(0.0055, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(38.0354, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([25, 30])\n",
      "step 9\n",
      "loss tensor(0.0056, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(35.6890, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([25, 30])\n",
      "step 10\n",
      "loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(32.3585, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([25, 30])\n",
      "step 11\n",
      "loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(30.0264, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([28, 30])\n",
      "step 12\n",
      "loss tensor(0.0052, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(39.5568, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(27)\n",
      "torch.Size([26, 30])\n",
      "step 13\n",
      "loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(27.9819, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([29, 30])\n",
      "step 14\n",
      "loss tensor(0.0036, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(38.2670, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(29)\n",
      "torch.Size([30, 30])\n",
      "step 15\n",
      "loss tensor(0.0052, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(40.8256, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(29)\n",
      "torch.Size([25, 30])\n",
      "step 16\n",
      "loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(30.9914, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([33, 30])\n",
      "step 17\n",
      "loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(42.6493, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(33)\n",
      "torch.Size([29, 30])\n",
      "step 18\n",
      "loss tensor(0.0053, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(36.2124, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(28)\n",
      "torch.Size([29, 30])\n",
      "step 19\n",
      "loss tensor(0.0045, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(39.2030, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(29)\n",
      "torch.Size([25, 30])\n",
      "step 20\n",
      "loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(28.9830, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([28, 30])\n",
      "step 21\n",
      "loss tensor(0.0029, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(31.5781, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(28)\n",
      "torch.Size([25, 30])\n",
      "step 22\n",
      "loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(30.9456, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([26, 30])\n",
      "step 23\n",
      "loss tensor(0.0041, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(32.4321, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([28, 30])\n",
      "step 24\n",
      "loss tensor(0.0038, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(34.9850, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(28)\n",
      "Accurate_total tensor(631)\n",
      "Distance_total tensor(802.6943, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch_accuracy 0.9952681388012619\n",
      "Epoch_distance_avg 1.2660793899396372\n",
      "Saving..\n",
      "Epoch 26\n",
      "===========\n",
      "torch.Size([28, 30])\n",
      "step 1\n",
      "loss tensor(0.0047, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(36.3349, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(27)\n",
      "torch.Size([25, 30])\n",
      "step 2\n",
      "loss tensor(0.0031, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(28.9136, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([13, 30])\n",
      "step 3\n",
      "loss tensor(0.0047, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(18.3954, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(13)\n",
      "torch.Size([25, 30])\n",
      "step 4\n",
      "loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(29.8324, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([25, 30])\n",
      "step 5\n",
      "loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(30.3550, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([25, 30])\n",
      "step 6\n",
      "loss tensor(0.0046, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(33.5442, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([32, 30])\n",
      "step 7\n",
      "loss tensor(0.0039, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(39.0351, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(32)\n",
      "torch.Size([26, 30])\n",
      "step 8\n",
      "loss tensor(0.0032, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(30.9966, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([28, 30])\n",
      "step 9\n",
      "loss tensor(0.0025, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(30.1053, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(28)\n",
      "torch.Size([25, 30])\n",
      "step 10\n",
      "loss tensor(0.0042, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(33.1578, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([26, 30])\n",
      "step 11\n",
      "loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(32.1772, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([25, 30])\n",
      "step 12\n",
      "loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(27.3933, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([26, 30])\n",
      "step 13\n",
      "loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(29.4328, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([25, 30])\n",
      "step 14\n",
      "loss tensor(0.0060, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(33.8034, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([25, 30])\n",
      "step 15\n",
      "loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(30.1781, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([29, 30])\n",
      "step 16\n",
      "loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(32.8865, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(29)\n",
      "torch.Size([25, 30])\n",
      "step 17\n",
      "loss tensor(0.0028, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(28.6052, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(25)\n",
      "torch.Size([33, 30])\n",
      "step 18\n",
      "loss tensor(0.0037, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(40.6528, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(33)\n",
      "torch.Size([29, 30])\n",
      "step 19\n",
      "loss tensor(0.0035, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(35.8292, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(29)\n",
      "torch.Size([26, 30])\n",
      "step 20\n",
      "loss tensor(0.0027, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(27.6686, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([29, 30])\n",
      "step 21\n",
      "loss tensor(0.0052, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(36.0792, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(27)\n",
      "torch.Size([30, 30])\n",
      "step 22\n",
      "loss tensor(0.0044, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(39.8736, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(30)\n",
      "torch.Size([26, 30])\n",
      "step 23\n",
      "loss tensor(0.0033, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(32.5971, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(26)\n",
      "torch.Size([28, 30])\n",
      "step 24\n",
      "loss tensor(0.0034, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "step_accuracy_distance tensor(34.6651, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "step_accurate tensor(28)\n",
      "Accurate_total tensor(631)\n",
      "Distance_total tensor(772.5123, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch_accuracy 0.9952681388012619\n",
      "Epoch_distance_avg 1.2184737204476213\n",
      "Saving..\n"
     ]
    }
   ],
   "source": [
    "encoder.train()\n",
    "# _hidden = encoder.initHidden()\n",
    "# Group batches/paddings by relative same-size\n",
    "\n",
    "accuracy_floor = .6\n",
    "distance_floor = 3.6\n",
    "\n",
    "for epoch in range(25,27):\n",
    "    \n",
    "    # modify for ordering -- benefit here with shuffling at ea epoch\n",
    "#     random.shuffle(train_ix)\n",
    "    random.shuffle(batches)\n",
    "    \n",
    "    print('Epoch',epoch)\n",
    "    print('===========')\n",
    "    step = 0\n",
    "    \n",
    "    distance = 0\n",
    "    accurate = 0\n",
    "    \n",
    "    # in future, mix up epochs \n",
    "    for i in range(len(batches)):\n",
    "        batch = batches[i]\n",
    "#     for i in range(19):\n",
    "        \n",
    "        step += 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "#         x,y,targets = get_minibatch(batchsize=BATCH_SIZE)\n",
    "        x,y,targets = get_minibatch(batchsize=len(batch))\n",
    "#         x = pad_minibatch(x)\n",
    "        \n",
    "#         y = torch.stack(y).double()\n",
    "        \n",
    "#         _hidden = encoder.initHidden(batch_size=BATCH_SIZE)\n",
    "        _hidden = encoder.initHidden(batch_size=len(batch))\n",
    "\n",
    "        yhat, _hidden = encoder(x, _hidden)\n",
    "\n",
    "        loss = loss_func(yhat,y)\n",
    "\n",
    "        loss.backward()\n",
    "#         loss.backward(retain_graph=True)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "       \n",
    "        print('step',step)\n",
    "        print('loss',loss)\n",
    "\n",
    "        yhat_tensor = torch.tensor([out_to_score_proba(hat) for hat in yhat])\n",
    "        \n",
    "        _stepdistance = accuracy_distansum(yhat_tensor=yhat_tensor,y_tensor=targets)\n",
    "        distance += _stepdistance\n",
    "        \n",
    "        _stepaccurate = accuracy(yhat_tensor=yhat_tensor,y_tensor=targets)\n",
    "        accurate += _stepaccurate\n",
    "        \n",
    "        \n",
    "        print('step_accuracy_distance',_stepdistance)\n",
    "        print('step_accurate',_stepaccurate)\n",
    "    \n",
    "    print('Accurate_total',accurate)\n",
    "    print('Distance_total',distance)\n",
    "    \n",
    "    epoch_accuracy = accurate.item()/(len(train_ix))\n",
    "    # may be a bug in distance..\n",
    "    epoch_distance_avg = distance.item()/(len(train_ix))\n",
    "    \n",
    "    print('Epoch_accuracy',epoch_accuracy)\n",
    "    print('Epoch_distance_avg',epoch_distance_avg)\n",
    "    \n",
    "    if epoch_accuracy > accuracy_floor or epoch_distance_avg < distance_floor:\n",
    "        \n",
    "        print('Saving..')\n",
    "        torch.save(encoder.state_dict(),'encoder_accuracy_{:.3f}_avgdistance_{:3f}.pt'.format(epoch_accuracy, epoch_distance_avg))\n",
    "        \n",
    "        if epoch_accuracy > accuracy_floor: accuracy_floor = epoch_accuracy\n",
    "        if epoch_distance_avg > distance_floor: distance_floor = epoch_distance_avg\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([158, 30])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder.eval()\n",
    "\n",
    "x_valid,y_valid,targets_valid = get_minibatch(ix=valid_ix)\n",
    "\n",
    "# x_valid = pad_minibatch(x_valid)\n",
    "\n",
    "# y_valid = torch.stack(y_valid).double()\n",
    "\n",
    "_hidden = encoder.initHidden(batch_size=len(valid_ix))\n",
    "\n",
    "yhat_valid, _hidden = encoder(x_valid, _hidden)\n",
    "estimates = torch.tensor([out_to_score_proba(hat) for hat in yhat_valid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30, 18, 29, 28, 26, 27, 23, 30, 19, 17, 20, 17, 25, 30, 23, 29, 27, 30,\n",
       "        19, 16, 30,  7, 28, 28, 28, 28, 25, 12, 30, 18, 30, 25, 15, 18, 14, 26,\n",
       "        27, 30, 20, 17, 30, 26, 19, 27, 17, 20, 20, 29, 27, 29, 30, 30, 30, 20,\n",
       "        22, 29, 19, 30, 30, 30, 30, 22, 17, 27, 27, 30, 30, 23, 30, 30, 19, 25,\n",
       "        30, 19, 30, 30, 30,  8, 30, 22, 30, 30, 28, 16, 19, 22, 24, 28, 16, 30,\n",
       "        30, 19, 17, 27, 19, 30, 30, 21, 28, 19, 29, 19, 12, 28, 20, 23, 13, 20,\n",
       "        23, 27, 23, 19, 20, 19, 19, 19, 27, 16, 10, 20, 17, 24, 24, 23, 16, 20,\n",
       "        18, 17, 19, 28, 29, 20, 27, 20, 23, 16, 20, 23, 27, 22, 19, 20, 20, 19,\n",
       "        19, 28, 28, 28, 26, 19, 24, 22, 20, 18, 23, 27, 25, 25])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30, 18, 29, 30, 26, 27, 23, 30, 19, 17, 20, 17, 25, 30, 23, 29, 27, 30,\n",
       "        19, 16, 30,  7, 28, 28, 28, 28, 25, 12, 30, 18, 30, 25, 15, 18, 14, 26,\n",
       "        27, 30, 20, 17, 30, 26, 19, 27, 17, 20, 20, 29, 27, 29, 30, 30, 30, 20,\n",
       "        22, 29, 19, 30, 30, 30, 30, 22, 17, 27, 27, 30, 30, 23, 30, 30, 19, 25,\n",
       "        30, 19, 30, 30, 30,  8, 30, 22, 30, 30, 28, 16, 19, 22, 24, 28, 16, 30,\n",
       "        30, 19, 17, 27, 19, 30, 30, 21, 28, 19, 29, 19, 12, 28, 20, 23, 13, 20,\n",
       "        23, 27, 23, 19, 20, 19, 19, 19, 27, 16, 10, 20, 17, 24, 24, 23, 16, 20,\n",
       "        18, 17, 19, 28, 29, 20, 27, 20, 23, 16, 20, 23, 27, 22, 19, 20, 20, 19,\n",
       "        19, 28, 28, 28, 26, 19, 24, 22, 20, 18, 23, 27, 25, 25])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export for heatmap visualization\n",
    "# pd.Series(targets_valid).to_csv('targets_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(estimates).to_csv('estimates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.6159, dtype=torch.float64, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is WRONG with this metric\n",
    "accuracy_distansum(targets_valid,estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09250569620253164"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14.6159/158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(157)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(targets_valid,estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9936708860759493"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "157/158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3]),)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(targets_valid != estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([634, 30])\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "\n",
    "x_train,y_train,targets_train = get_minibatch(ix=train_ix)\n",
    "\n",
    "# x_valid = pad_minibatch(x_valid)\n",
    "\n",
    "# y_valid = torch.stack(y_valid).double()\n",
    "\n",
    "_hidden = encoder.initHidden(batch_size=len(train_ix))\n",
    "yhat_train, _hidden = encoder(x_train, _hidden)\n",
    "estimates_train = torch.tensor([out_to_score_proba(hat) for hat in yhat_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28, 30, 12, 17, 30, 29, 20, 29, 15, 30, 29, 29, 29, 29, 30, 30, 28,  3,\n",
       "        28, 27, 30, 19, 27, 30, 10, 30, 30, 17, 23, 30, 30, 19, 30, 30, 30, 30,\n",
       "        30, 28, 30, 19, 29, 30, 30, 29, 30, 11, 27, 21, 19, 29, 30, 28, 30, 12,\n",
       "        30, 13, 22, 21, 29, 29, 17, 10, 23, 13, 18, 11, 28, 30, 18,  3, 29, 27,\n",
       "        30, 21, 20, 25, 30, 28, 30, 17, 24, 26, 20, 27, 30, 30, 19, 28, 24, 25,\n",
       "        20, 24, 30, 29, 16, 30, 30, 30, 30, 30, 19, 28, 15, 30, 30, 26, 30, 29,\n",
       "        30, 30, 30, 26, 30, 19, 26, 23, 18, 19, 13, 23, 12, 25, 30, 30, 10, 24,\n",
       "        30, 30, 28, 28, 15, 30, 30, 30, 24, 29, 28, 30, 28, 30, 23, 23, 20, 30,\n",
       "        29,  8, 20, 30, 19, 20, 25, 29, 20, 30, 30, 28, 30, 27, 29, 28, 22, 28,\n",
       "        25, 23, 29, 20, 23, 16, 18, 23, 19, 25, 30, 30, 17, 30, 30, 20, 10, 30,\n",
       "        18, 22, 24, 15, 29, 30, 19, 18, 28, 30, 30, 29, 30, 30, 30, 22, 29, 30,\n",
       "        29, 30, 26, 29, 15, 16, 30, 30, 13, 24, 30, 29, 23, 28, 13, 30, 28, 11,\n",
       "        29, 18, 18, 17, 27, 16, 12, 13, 29, 25, 30, 15, 30, 29, 10, 17, 11, 19,\n",
       "        10, 27, 22, 19, 28, 29, 28, 30, 19, 30, 26, 25, 13, 30, 22, 28, 29, 30,\n",
       "        13, 18, 23, 30, 24, 14, 30, 16, 30, 23, 30, 30, 15, 30, 30, 14, 18, 19,\n",
       "        10, 19, 24, 28, 19, 19, 30, 11, 28, 30, 29, 13, 17, 30, 12, 18, 18, 19,\n",
       "        30, 26, 29, 15, 17, 29, 25, 20, 28, 18, 30, 10, 26, 23, 21, 29, 19, 20,\n",
       "        20, 19, 20, 18, 29, 13, 30, 16, 28, 30, 29, 30, 17, 20, 24, 23, 23, 23,\n",
       "        30, 30, 30, 23, 30, 30, 27, 30, 19, 30, 18, 14, 29, 29, 27, 29, 29, 30,\n",
       "        19, 30, 17, 30, 28, 13, 14, 20, 30, 24, 15, 23, 30, 12, 30, 24, 29, 20,\n",
       "        30, 30, 30, 29, 14, 26, 29, 15, 19, 30, 20, 20, 30, 20, 28, 26, 29, 19,\n",
       "        22, 28, 29, 17, 29, 28, 12, 20, 29, 17,  5, 17, 30, 30, 29, 30, 29, 28,\n",
       "        30, 29, 25, 30, 24, 29, 21, 24, 30, 19, 19, 30, 25, 28, 17, 30,  1, 27,\n",
       "        12, 20, 30, 24, 27, 30, 27, 28, 28, 15, 19, 26, 20, 15, 26, 30, 28, 19,\n",
       "        30, 27, 30, 19, 30, 13, 28, 30, 28, 21, 20, 20, 30, 19, 13, 13, 21, 19,\n",
       "        29, 19, 19, 24, 17, 19, 13, 12, 26, 23, 18, 13, 15, 25, 20, 28, 21, 21,\n",
       "        16, 29, 24, 21, 17,  7, 29, 19, 16, 21, 19, 29, 20,  8, 26, 28, 18, 25,\n",
       "        11, 21, 23, 18, 17, 28, 19, 19, 28,  8, 23, 30, 19, 27, 17, 13, 28, 26,\n",
       "        23, 26, 28, 24, 19, 19, 15, 10, 20, 17, 27, 11, 30, 28, 12, 10, 22, 19,\n",
       "        28, 19, 22, 13, 11, 25, 11, 14, 17, 30, 21, 20, 24, 15, 18, 26, 12, 19,\n",
       "        23, 19, 11, 21, 26, 19, 25, 17,  7, 22, 30, 15, 13, 27, 23, 25, 25, 17,\n",
       "        19, 23, 27, 19, 23, 21, 13, 18, 18, 19, 19, 29, 25, 10, 28, 20, 29, 29,\n",
       "        30, 24, 27, 19, 20, 14, 19, 12, 27, 19, 23, 17, 18, 20, 13, 17, 14, 25,\n",
       "        14, 22, 20, 28, 29, 17, 18, 18, 29, 18, 23, 19, 19, 28, 25, 24, 19, 15,\n",
       "        13, 13, 20, 19, 20, 19, 26, 17, 28, 19, 19, 15, 19, 29, 18, 18, 10, 15,\n",
       "        30, 15, 25, 17])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates_train_bugfix = torch.tensor([out_to_score_proba(hat) for hat in yhat_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28, 30, 12, 17, 30, 29, 20, 29, 15, 30, 29, 29, 29, 29, 30, 30, 28,  3,\n",
       "        28, 27, 30, 19, 27, 30, 10, 30, 30, 17, 23, 30, 30, 19, 30, 30, 30, 30,\n",
       "        30, 28, 30, 19, 29, 30, 30, 29, 30, 11, 27, 21, 19, 29, 30, 28, 30, 12,\n",
       "        30, 13, 22, 21, 29, 29, 17, 10, 23, 13, 18, 11, 28, 30, 18,  3, 29, 27,\n",
       "        30, 21, 20, 25, 30, 28, 30, 17, 24, 26, 20, 27, 30, 30, 19, 28, 24, 25,\n",
       "        20, 24, 30, 29, 16, 30, 30, 30, 30, 30, 19, 28, 15, 30, 30, 26, 30, 29,\n",
       "        30, 30, 30, 26, 30, 19, 26, 23, 18, 19, 13, 23, 12, 25, 30, 30, 10, 24,\n",
       "        30, 30, 28, 28, 15, 30, 30, 30, 24, 29, 28, 30, 28, 30, 23, 23, 20, 30,\n",
       "        29,  8, 20, 30, 19, 20, 25, 29, 20, 30, 30, 28, 30, 27, 29, 28, 22, 28,\n",
       "        25, 23, 29, 20, 23, 16, 18, 23, 19, 25, 30, 30, 17, 30, 30, 20, 10, 30,\n",
       "        18, 22, 24, 15, 29, 30, 19, 18, 28, 30, 30, 29, 30, 30, 30, 22, 29, 30,\n",
       "        29, 30, 26, 29, 15, 16, 30, 30, 13, 24, 30, 29, 23, 28, 13, 30, 28, 11,\n",
       "        29, 18, 18, 17, 27, 16, 12, 13, 29, 25, 30, 15, 30, 29, 10, 17, 11, 19,\n",
       "        10, 27, 22, 19, 28, 29, 28, 30, 19, 30, 26, 25, 13, 30, 22, 28, 29, 30,\n",
       "        13, 18, 23, 30, 24, 14, 30, 16, 30, 23, 30, 30, 15, 30, 30, 14, 18, 19,\n",
       "        10, 19, 24, 28, 19, 19, 30, 11, 28, 30, 29, 13, 17, 30, 12, 18, 18, 19,\n",
       "        30, 26, 29, 15, 17, 29, 25, 20, 28, 18, 30, 10, 26, 23, 21, 29, 19, 20,\n",
       "        20, 19, 20, 18, 29, 13, 30, 16, 28, 30, 29, 30, 17, 20, 24, 23, 23, 23,\n",
       "        30, 30, 30, 23, 30, 30, 27, 30, 19, 30, 18, 14, 29, 29, 27, 29, 29, 30,\n",
       "        19, 30, 17, 30, 28, 13, 14, 20, 30, 24, 15, 23, 30, 12, 30, 24, 29, 20,\n",
       "        30, 30, 30, 29, 14, 26, 29, 15, 19, 30, 20, 20, 30, 20, 28, 26, 29, 19,\n",
       "        22, 28, 29, 17, 29, 28, 12, 20, 29, 17,  5, 17, 30, 30, 29, 30, 29, 28,\n",
       "        30, 29, 25, 30, 24, 29, 21, 24, 30, 19, 19, 30, 25, 28, 17, 30,  1, 27,\n",
       "        12, 20, 30, 24, 27, 30, 27, 28, 28, 15, 19, 26, 20, 15, 26, 30, 28, 19,\n",
       "        30, 27, 30, 19, 30, 13, 28, 30, 28, 21, 20, 20, 30, 19, 13, 13, 21, 19,\n",
       "        29, 19, 19, 24, 17, 19, 13, 12, 26, 23, 18, 13, 15, 25, 20, 28, 21, 21,\n",
       "        16, 29, 24, 21, 17,  7, 29, 19, 16, 21, 19, 29, 20,  8, 26, 28, 18, 25,\n",
       "        11, 21, 23, 18, 17, 28, 19, 19, 28,  8, 23, 30, 19, 27, 17, 13, 28, 26,\n",
       "        23, 26, 28, 24, 19, 19, 15, 10, 20, 17, 27, 11, 30, 28, 12, 10, 22, 19,\n",
       "        28, 19, 19, 13, 11, 25, 11, 14, 17, 30, 21, 20, 24, 15, 18, 26, 12, 19,\n",
       "        23, 19, 11, 21, 26, 19, 25, 17,  7, 22, 30, 15, 13, 27, 23, 25, 25, 17,\n",
       "        19, 23, 27, 19, 23, 21, 13, 18, 18, 19, 19, 29, 25, 10, 28, 20, 29, 29,\n",
       "        30, 23, 27, 19, 20, 14, 19, 12, 27, 19, 23, 17, 18, 20, 13, 17, 14, 25,\n",
       "        14, 22, 20, 28, 29, 17, 18, 18, 29, 18, 23, 19, 19, 28, 25, 24, 19, 15,\n",
       "        13, 13, 20, 19, 20, 19, 26, 17, 28, 19, 19, 15, 19, 29, 18, 18, 10, 15,\n",
       "        30, 15, 25, 17])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimates_train_bugfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(632)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(targets_train,estimates_train_bugfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9968454258675079"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "632/len(train_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.6159, dtype=torch.float64, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_distansum(targets_train,estimates_train_bugfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02305347003154574"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14.6159/len(train_ix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python dl1010",
   "language": "python",
   "name": "dl1010"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
